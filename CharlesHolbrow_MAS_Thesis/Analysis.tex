\clearpage
\chapter{Discussion and Analysis}
\label{ch:analysis}
In the previous chapters we explored three new tools for creating and
processing music, including their motivations and implementations.
\polytempic in \autoref{ch:polytempic} proposed a mathematical
approach for composing previously inaccessible polytempic music. The
\refmod in \autoref{ch:ref-mod} introduced an interface for quickly
sketching abstract architectural and musical ideas.  Chapters
\ref{ch:hypercompressor} and \ref{ch:experience} described the
motivations for an implementation of a new technique for moving music in
space and time. Each of these projects builds on Iannis Xenakis'
theory of \textit{stochastic music} and incorporates elements from
other disciplines, including mathematics, computer science, acoustics,
audio engineering and mixing, sound reinforcement, multimedia
production, and live performance.

This final chapter discusses how each project succeeded, how each
project failed, and how future iterations can benefit from lessons
learned during the development process.

\section{Evaluation Criteria}
\label{sec:eval-criteria}
To evaluate a project of any kind, it is helpful to begin with a
purpose, and then determine the granularity and scope of the
evaluation.\cite{Saltzer2009} We might evaluate a music recording for
audio fidelity, for musical proficiency of the artist, for emotional
impact or resonance, for narrative, for technological inovation, for
creative vision, or for political and historical insight. Similarly,
we can evaluate the suitability of an analog to digital converter
(ADC) for a given purpose. If our purpose is music recording, we might
prefer different qualities than if our purpose is electrical
engineering. A recording engineer might prefer that the device impart a
favorable sound, while an acoustician may prefer that the device be as
neutral as possible.

In the evaluation of a music recording, and the evaluation of an ADC,
we concern ourselves with only the highest level interface: When
evaluating a music recording, we listen to the sound of the
recording, but we do not evaluate the performance of the ADC used to
make the recording. Evaluation is simplified when we consider fewer
levels of abstraction.

Stochastic music theory is a \textit{vertical integration} of
mathematics, the physics of sound, psychoacoustics, and music. The
theory of stochastic music begins with the lowest level components of
sound and ends with a creative musical product. What is a reasonable
perspective from which to evaluate stochastic music? From the
perspective of listening or performing the music? From the
perspective of a historian, evaluating the environment that led to the
composition or studying the impact on music afterwards?  Should we
try to make sense of the entire technology stack, or try to evaluate
every layer of abstraction individually? Somehow, between the
low-level elements of sound and a musical composition or performance,
we transition from what is numerically quantifiable to what we can
only attempt to describe.

In my evaluation, I focus on two qualities. First, I study how each
project achieved its original objectives and how it fell short. Second,
I consider how each project can influence or inspire future
iterations. I avoid comparative analysis or evaluation based on any
kind of rubric. Instead, I evaluate the results of the project
according to its own motivations and historical precedents.

% Does the unit have an appropriate number and type of audio outputs
% suit our needs (such as TRS, XLR, and USB)? Does it perform
% reliably? Does it have a neutral or favorable impact on the sound?
% The results of our evaluation will depend on our purpose. If the
% purpose of the device is for playback of audio in a live performance
% context, the best choice will be different than if we want to use
% the unit for mixing in a recording studio.

\section{\polytempic}
This chapter presents a very pure and elegant solution to a very
complex problem. But is it important? Is it a significant improvement
on the existing techniques presented in section
\ref{sec:background-polytempi}? If a performer cannot play precise
tempo curves anyway, what is this actually for?

Western polytempic music as defined in \autoref{ch:polytempic} has
existed for only slightly over one century, and there is certainly
room for new explorations. The oldest example of Western polytempic
music is by Charles Ives in his 1906 piece, \textit{Central Park in
  the Dark}.\cite{Greschak2003} In the piece, the string section
represents nighttime darkness, while the rest of the orchestra
interprets the sounds of Central Park at night. Beginning at measure
64, Ives leaves a note in the score, describing how the orchestra
accelerates, while the string section continues at a constant tempo:
\begin{quotation}
  From measure 64 on, until the rest of the orchestra has played
  measure 118, the relation of the string orchestra's measures to
  those of the other instruments need not and cannot be written down
  exactly, as the gradual accelerando of all but the strings cannot be
  played in precisely the same tempi each time.
\end{quotation}
Ives acknowledges that there is no existing notation to exactly
describe the effect that he wants, and that musicians are not capable
of playing the transition in a precise way. In this example, it is not
important that the simultaneous tempi have a precise rhythmic
relationship. Ives' use of parallel tempi is a graceful one. He
achieves a particular effect without requiring the musicians to do
something as difficult as accelerate and decelerate relative to each
other, and then resynchronize at certain points.

All polytempic compositions must grapple with the issue of
synchronicity, and many demand more precision than \textit{Central
  Park in the Dark}. Stockhausen's \textit{Gruppen} uses polytempi
very aggressively, going to great lengths to ensure that the three
orchestras rhythmically synchronize and desynchronize in just the
right way. If Stockhausen had been able to control the synchronicity
of the tempo precisely, it seems likely that he would have wanted to
try it.

Some music (and perhaps stochastic music in particular) may be more
interesting or influential from a theoretical perspective, than for
the music itself in isolation. It could be that the possibilities
unlocked through the equations derived in \autoref{ch:polytempic} are
not different enough from the approximations used by Nancarrow and
Cage or that it is unrealistic to direct performers to play them
accurately enough to perceive the difference.

However it is surprising that current digital tools for composition do
not let us even \emph{try} \polytempic. If we want to hear what tempo
transitions like the ones describe here sound like using digital
technology, there is no software that lets us do so, and we are still
forced to approximate. Audio programming languages like Max and
SuperCollider let us code formulaic tempi into our compositions, but
equations like the ones derived here are still required. I could not
find any technique that lets us create swarms of tempo
accelerations that fit the constraints described in
\autoref{ch:polytempic}, or any musical example that proposed to have
found another solution.

For some cases approximation is perfectly acceptable. If a musician
is incapable of playing the part, we are also likely incapable of
hearing the subtleties that distinguish an approximation from a
perfect performance. However, if we want large collections of
simultaneous polytempi, like the ones shown in
figures~\ref{fig:polytempic-transition}
and~\ref{fig:polytempic-transition-3}, the approximations possible
with transcriptions, or the approximations of an unassisted human
performers, are not precise enough.

\paragraph{Next Steps} Bryn Bliska's composition (linked in
section~\ref{sec:composition}) is a good starting point for future
explorations, but it was composed with an earlier version of the
polytempic equation that did not allow for large swarms of tempi. In
its current state, the polytempic work described in this thesis is
just a beginning. We have not yet tried to compose a piece that
fully incorporates \polytempic. 

Equations alone do not make a musical instrument, and composition is
difficult without a musical interface. There are a few modern
examples of polytempic projects (see \autoref{ch:polytempic}), but I
could not find any examples of interfaces for composing with
coordinated mass of tempi. The most exciting direction for
this project is the creation of new musical interfaces for composing
and manipulating stochastic tempo swarms. 


\section{\refmod}
This project provides a single abstract interface that approaches
composition of space (architecture) and the composition of music at
the same time. The forms that it makes are familiar from the ruled
surfaces seen in Xenakis' compositions and early sketches of the
Philips Pavilion. From a musical perspective, we can think of the x
and y axes representing time and pitch. From an architectural
perspective the canvas might represent the floor plan of spaces we are
designing.

While it is interesting to switch our perspective between the two
modes, there is not a clear connection from one to the other. A
carefully designed surface or reflection in one mode would be quite
arbitrary in the other mode. The reason that the interface is capable
of working in both modes is because it is so abstract that it does
not commit to one or the other. This is not a complete failing: The
tool was really designed to be a brainstorming aid at the very
beginning of the design process. It can be much simpler and quicker to
use than proper architectural software as a means of creating
abstract shapes, similar to sketching on paper, before turning to
specialized software for more detailed design.

\paragraph{Curves, Constraints, and Simplicity} Despite the
limitations of this project, the parts that worked well do form a
strong base for future iterations. There is something simple and
\textit{fun} about the user interface. There is only one input action;
dragging a control point. It is immediately clear what each control
point does when it is moved. It is easy to not even notice that there
are five different types of control points and each has slightly
different behavior. It is very intuitive to adjust a reflection
surface such that the red beams \textit{focus} on a certain point, and
then re-adjust a reflection surface so that they diverge
chaotically. There is something fascinating about how the simple
movements intuitively produce coordinated or chaotic stochastic
results.

The red ``sound lines'' have three degrees of freedom: position,
direction, and length. We can point the rays in any direction we like,
but their movement is somewhat constrained.  The projection angle is
locked to 30 degrees and the number of beams is always eight, and
most of the flexibility from the interface comes from the reflective
surfaces.

\paragraph{Stochastic by Default} The \refmod interface makes it easier
to draw a curving reflective surface than a straight one. If you make
a special effort, it is possible to make one of the surfaces straight,
but just like drawing a line on a paper with a pen, curved surfaces
come more naturally. The curves in the \refmod do not come naturally
because they are following an input gesture like most ``drawing''
interfaces, but because of the simple mathematics in the of the Bezier
curves. If we consider the red lines to be notes on a time/pitch axis,
the default interpretation is stochastic glissandi rather than static
pitches. Most musical software assumes static pitches by default and
most architectural software assumes straight lines.

\paragraph{Next Steps}
The obvious next steps for this project involve correcting the
shortcomings described above. It could be made to work in three
dimensions, and model precise propagation of sound rather than a very
simplified abstraction: It could become a proper acoustical
simulator. Another possibility is turning it into a compositional or
performative musical instrument where we can hear the stochastic
glissandi in realtime. These options are not necessarily mutually
exclusive, but as the interface becomes tailored to a more specific
application, our ability to think about the content as abstract
representations also breaks down. The ideal of software that is
equally well-equipped to compose music and to imagine architectural
spaces is probably unrealistic. 

Any visual representation of music is quite abstract, and different
visual representations can encourage us to think about music in new
and unusual ways. For example each red line can be considered pitch,
but it can also be considered its own time axis. By calculating the
red paths, we can creating many time axes that follow similar but
slightly different trajectories. Alternatively, each red line can be thought of as a
time axis for an individual pitch. When the lines collide with a
curved surface after slightly different lengths, it represents an
arpeggiated chord. In contrast, a non-arpeggiated chord is represented
when the red lines all collide with a surface after traveling
identical distances. The abstract nature of this interface leaves room
for our imagination to interpret unexpected new musical
possibilities. 


\section{\thesis}
The design and development of \thesis happened in parallel with
pre-production for \textit{De L'Exp\'{e}rience}, and the
Hypercompressor was, in part, tailored to the needs of a somewhat
unique situation. The resulting project leaves significant design
questions surrounding ambisonic dynamic range compression unanswered.
For example: What is the best way to detect and attenuate a region on
our surround sphere that is an unusual or elongated shape?  Should the
compressor attempt to attenuate the narrow region only?  Should we
attenuate the center of the region more than the edge?


When a region of our surround sound image exceeds the
Hypercompressor's threshold, the compressor warps the surround image
in addition to attenuating the region where the threshold overage
occurred. This makes sense for side-chain compression, but is less
applicable to standard compression. We could have chosen only
warping, or only attenuation, each of which represents its own
compromise:
\begin{itemize}
\item We could simply warp all sounds away from a region that exceeds
  the compression threshold without attenuating them at all. However,
  doing so would increase the perceived level of the sound coming from
  the opposite direction. We also run the risk of creating sonic
  ``ping-pong'' of sounds arbitrarily panning. This can sound
  exciting, but quickly becomes a contrivance or gimmick. 
\item If we simply attenuate a region that exceeds the threshold, we
  are not taking advantage of the opportunities provided to us by
  surround sound in the first place. In side-chain mode, we risk
  hiding a compressed sound completely when we could simply warp that
  region of the surround field to a location where it can be heard
  more clearly.
\end{itemize}
The current implementation also does not handle the case when two
separate regions of the surround field both exceed the threshold.

\paragraph{\textit{De L'Exp\'{e}rience}}
The main goal of using the Hypercompressor was to blend the electronic
textures with the sound of the Pierre B\'{e}ique organ in Tod
Machover's composition. The chosen approach was to give the
electronics a sense of motion that the organ (whose sound is
awe-inspiring, but also somewhat static) cannot produce; thus the
electronics can be heard moving \emph{around} the sound of the organ,
rather than being required to compete with the sound of the organ.
The first attempt at this goal, however, did not go as planned.

The electronics were mixed to occupy as much of the surround sound
sphere as possible, filling the entire room with sound.  My original
idea was to spatially separate the organ and electronics by connecting
them to the Hypercompressor in side-chain mode.  When the organ was
playing it would \emph{push} the sound of the electronics to the back
of the room, making it easier to hear both timbres without either
masking the other.  During the \textit{De L'Exp\'{e}rience} rehearsal,
this was the first approach I tried, but the resulting surround
texture had a different problem: The sound of the organ and the sound
of the electronics were \emph{too} separate. They did not blend with
each other in space, but existed as two clearly distinct sources. I
arrived at the solution described in \autoref{ch:hypercompressor} only
after first trying the exact opposite of the final approach. While I
had to revise my strategy during the rehearsal I consider the
Hypercompressor to have aided the blending of the organ and
electronics especially well. It is important to note that the
beautiful blend of sounds that we achieved would have been possible
without many other contributing factors, such as the expert
composition of the electronic textures.


\section{Stochos}
\thesis is a complete realization of a musical idea. Beginning with an
objective and a mathematical foundation, we designed and built a custom
software implementation and applied it in a live performance
context. A study of the process has revealed what is probably the
greatest strength of stochastic music theory: The vertical integration
of the theory of sound and music lets us study music from a privileged
perspective, while the controlled chance built into the system helps
us to uncover possibilites that could not be found with conventional
means. In the case of \thesis, we move sounds in space based on matrix
transforms, that are themselves driven by the controlled chance of a
random performance. The angular position of our sounds are defined by
both explicit mathematical formulas and the unpredictable qualities of
live performance.

From a broad perspective all three projects emerged ``by chance'' in
the same way. Each one is the result of musical exploration in a space
that indiscriminately draws from mathematics, computer science,
acoustics, audio engineering and mixing, sound reinforcement,
multimedia production, and live performance. By treating all these
disciplines as components of music theory, we discover new musical
patterns and possibilities for shaping sound in time and space.

% The positioning sounds in space 
% We have discussed the advantages and disadvantages of warping and
% attenuation as techniques for handling surround sound
% compression. Attenuation is the approach used most mono and stereo
% recordings, while surround sound lets us explore warping as an
% alternative. One obvious next step is to build a version that simple
% lets us toggle (or adjust continuously) between a warping and
% attenuation.

\clearpage
\chapter*{Epilogue}
\label{ch:epilogue}

% The grant was 99510\EUR{}
In 2004, the Culture 2000 Programme, created by the European Union
approved a grant to an Italian multimedia firm for a project called
Virtual Electronic Poem (VEP).\cite{eu2004} The project proposed the
creation of a virtual reality experience in which users could enter a
simulated version of the famous Phillips Pavilion. While developing
the VEP, the design team went through the archives of Xenakis, Le
Corbusier, and Philips, uncovering every relevant bit of information
in order to make the experience as real as
possible.\cite{Lombardo2009}

Virtual reality technology changed so much between 2004 and 2015 that
reviving the VEP project today would likely involve an additional
multimedia archeology expedition as intensive as the first: It would
probably be easier (and more effective) to start from scratch using
the original documentation. A common problem with multimedia
performances is that technology changes so fase that it quickly
becomes very difficult to restore even moderately recent
projects.\cite{Lombardo2006} In contrast, the mathematical language
that Xenakis used to describe his work is as well-established as the
language of Western music notation, and for this reason we have a
surprisingly thorough understanding of his music today. It is my hope
that the documentation in this thesis will provide an equally
dependable and enduring a description of the process of modern musical
composition.



% Having mixed a bunch of stuff, for different contexts, the different
% panning strategies in discrete channel, ambisonics, don't are not
% better or worse. Stockhausen didn't really didn't need anything other than
% discrete channel panning for Gesang. In fact the piece holds up very
% well in stereo. I do think that there is a difference between actual
% surround and 5.1, the latter resembling dual stereo more than actual
% surround. 

% welp, not sure how well it actually
% worked. And I don't expect electronic instrument to ever replace
% acoustic instruments, although they may eventually equal them in
% expressivity. Initially thought it would work backwards

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "CharlesHolbrow_MAS_Thesis"
%%% End:
